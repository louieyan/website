---
title: Machine Learning Resources
author: Lei Yan
date: '2019-01-02'
slug: machine-learning-resources
categories:
  - Machine Learning
tags:
  - ML
  - NLP
---



<p>I will continue adding new good resources to this post. But donâ€™t get lost in them. Choose one and keep learning. After you finished one course, then choose another one.<br />
Hope this can be helpful to you!</p>
<div id="github-resources" class="section level1">
<h1>Github Resources</h1>
<div id="roadmap-of-dl-and-ml" class="section level2">
<h2><a href="https://github.com/L1aoXingyu/Roadmap-of-DL-and-ML">Roadmap of DL and ML</a></h2>
<p>For a newecomer to deep learning and machine learning area, facing some much courses and resources, the first question is how to choose right books and courses to begin this trip.The roadmap includes some highly recommended courses, books and papers which can help you get into DL and ML area quickly. Maybe some materials are quite difficult, but really worth reading and studying.</p>
</div>
<div id="machine-translation-reading-list" class="section level2">
<h2><a href="https://github.com/THUNLP-MT/MT-Reading-List">Machine Translation Reading List</a></h2>
<p>A machine translation reading list maintained by the Tsinghua Natural Language Processing Group.</p>
</div>
<div id="tensorflow-course" class="section level2">
<h2><a href="https://github.com/osforscience/TensorFlow-Course">TensorFlow Course</a></h2>
<p>This repository aims to provide simple and read-to-use tutorials for TensorFlow. Each tutorial includes <code>source code</code> and most of them are associated with a <code>documentation</code>.</p>
</div>
<div id="summer-school-on-deep-learning-and-bayesian-methods" class="section level2">
<h2><a href="http://deepbayes.ru/">Summer School on Deep Learning and Bayesian Methods</a></h2>
<p>In this course, they discuss how Bayesian Methods can be combined with Deep Learning and lead to better results in machine learning applications.<br />
The most important thing is this course provides full videos, slides and assignments.</p>
</div>
<div id="machine-learning-for-opencv" class="section level2">
<h2><a href="https://github.com/mbeyeler/opencv-machine-learning">Machine Learning for OpenCV</a></h2>
<p>It is a Jupyter notebook version of <a href="https://www.amazon.com/Machine-Learning-OpenCV-Michael-Beyeler/dp/1783980281">this book</a><br />
It might be a good book for the people who want to learning CV but with little background.</p>
</div>
<div id="kaggle-past-solution" class="section level2">
<h2><a href="https://github.com/EliotAndres/kaggle-past-solutions">Kaggle Past Solution</a></h2>
<p>A searchable and sortable compilation of <a href="https://www.kaggle.com/">Kaggle</a> past solutions. <a href="http://ndres.me/kaggle-past-solutions/">Website</a></p>
</div>
<div id="hands-on-machine-learning" class="section level2">
<h2><a href="https://github.com/ageron/handson-ml">Hands-On Machine Learning</a></h2>
<p>This project aims at teaching you the fundamentals of Machine Learning in python. It contains the example code and solutions to the exercises in <a href="http://shop.oreilly.com/product/0636920052289.do">this book</a>.<br />
<strong>You can download a pdf version of this book at <a href="https://pan.baidu.com/s/1fb_jjSXZG9Ixe4hgmoDEug">here</a>.</strong></p>
</div>
<div id="code-for-prml" class="section level2">
<h2><a href="https://github.com/PRML/PRMLT">Code for PRML</a></h2>
<p>This Matlab package implements machine learning algorithms described in the great textbook: Pattern Recognition and Machine Learning by C. Bishop (<a href="https://www.microsoft.com/en-us/research/people/cmbishop/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fum%2Fpeople%2Fcmbishop%2Fprml%2F">PRML</a>).</p>
</div>
<div id="cheatsheets-ai" class="section level2">
<h2><a href="https://github.com/kailashahirwar/cheatsheets-ai">Cheatsheets AI</a></h2>
<p>Essential Cheat Sheets for deep learning and machine learning researchers and practitioners.</p>
</div>
<div id="paper-with-code" class="section level2">
<h2><a href="https://paperswithcode.com/">Paper with code</a></h2>
<p>The mission of Papers With Code is to create a free and open resource with Machine Learning papers, code and evaluation tables.</p>
</div>
</div>
<div id="qa-of-some-concrete-topics" class="section level1">
<h1>Q&amp;A of Some Concrete Topics</h1>
<div id="maximum-likelihood-estimators---multivariate-gaussian" class="section level2">
<h2><a href="https://stats.stackexchange.com/questions/351549/maximum-likelihood-estimators-multivariate-gaussian/351550#351550?newreg=ca7f4cdfdd2948ca9589cbaece9409e7">Maximum Likelihood Estimators - Multivariate Gaussian</a></h2>
<p>The full derivation of the Maximum Likelihood Estimators for the multivariate Gaussian.</p>
</div>
<div id="why-is-the-cov-of-multivariate-gaussian-positive-definite" class="section level2">
<h2><a href="http://cs229.stanford.edu/section/gaussians.pdf">Why Is The Cov of Multivariate Gaussian Positive Definite?</a></h2>
<p>Page 3 of a note of CS229.</p>
</div>
<div id="why-we-use-negative-log-likelihood-sometimes" class="section level2">
<h2><a href="https://stats.stackexchange.com/questions/141087/i-am-wondering-why-we-use-negative-log-likelihood-sometimes">Why We Use Negative (log) Likelihood Sometimes?</a></h2>
<p>This question puzzled me when I read page 96 of PRML. The original book uses likelihood, but in the errata, the author changed likelihood to negative likelihood to statisfy to condition of Robbins and Monro(1951).</p>
</div>
</div>
