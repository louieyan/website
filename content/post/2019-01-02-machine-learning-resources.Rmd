---
title: Machine Learning Resources
author: Lei Yan
date: '2019-01-02'
slug: machine-learning-resources
categories:
  - Machine Learning
tags:
  - ML
  - NLP
---
I will continue adding new good resources to this post. But don't get lost in them. Choose one and keep learning. After you finished one course, then choose another one.  
Hope this can be helpful to you!  

# Github Resources

## [Roadmap of DL and ML](https://github.com/L1aoXingyu/Roadmap-of-DL-and-ML)  
For a newecomer to deep learning and machine learning area, facing some much courses and resources, the first question is how to choose right books and courses to begin this trip.The roadmap includes some highly recommended courses, books and papers which can help you get into DL and ML area quickly. Maybe some materials are quite difficult, but really worth reading and studying.  

## [Machine Translation Reading List](https://github.com/THUNLP-MT/MT-Reading-List)  
A machine translation reading list maintained by the Tsinghua Natural Language Processing Group.  

## [TensorFlow Course](https://github.com/osforscience/TensorFlow-Course)  
This repository aims to provide simple and read-to-use tutorials for TensorFlow. Each tutorial includes `source code` and most of them are associated with a `documentation`.   

## [Summer School on Deep Learning and Bayesian Methods](http://deepbayes.ru/)  
In this course, they discuss how Bayesian Methods can be combined with Deep Learning and lead to better results in machine learning applications.    
The most important thing is this course provides full videos, slides and assignments.    

## [Machine Learning for OpenCV](https://github.com/mbeyeler/opencv-machine-learning)  
It is a Jupyter notebook version of [this book](https://www.amazon.com/Machine-Learning-OpenCV-Michael-Beyeler/dp/1783980281)  
It might be a good book for the people who want to learning CV but with little background.  

## [Kaggle Past Solution](https://github.com/EliotAndres/kaggle-past-solutions)  
A searchable and sortable compilation of [Kaggle](https://www.kaggle.com/) past solutions. [Website](http://ndres.me/kaggle-past-solutions/)  

## [Hands-On Machine Learning](https://github.com/ageron/handson-ml)  
This project aims at teaching you the fundamentals of Machine Learning in python. It contains the example code and solutions to the exercises in [this book](http://shop.oreilly.com/product/0636920052289.do).  
**You can download a pdf version of this book at [here](https://pan.baidu.com/s/1fb_jjSXZG9Ixe4hgmoDEug).**  

## [Code for PRML](https://github.com/PRML/PRMLT)  
This Matlab package implements machine learning algorithms described in the great textbook: Pattern Recognition and Machine Learning by C. Bishop ([PRML](https://www.microsoft.com/en-us/research/people/cmbishop/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fum%2Fpeople%2Fcmbishop%2Fprml%2F)).  

## [Cheatsheets AI](https://github.com/kailashahirwar/cheatsheets-ai)
Essential Cheat Sheets for deep learning and machine learning researchers and practitioners.

## [Paper with code](https://paperswithcode.com/)
The mission of Papers With Code is to create a free and open resource with Machine Learning papers, code and evaluation tables.

## [Interpretable machine learning](https://christophm.github.io/interpretable-ml-book/)
This book explains to you how to make (supervised) machine learning models interpretable.

# Q&A of Some Concrete Topics

## [Maximum Likelihood Estimators - Multivariate Gaussian](https://stats.stackexchange.com/questions/351549/maximum-likelihood-estimators-multivariate-gaussian/351550#351550?newreg=ca7f4cdfdd2948ca9589cbaece9409e7)  
The full derivation of the Maximum Likelihood Estimators for the multivariate Gaussian.  

## [Why Is The Cov of Multivariate Gaussian Positive Definite?](http://cs229.stanford.edu/section/gaussians.pdf)    
Page 3 of a note of CS229.  

## [Why We Use Negative (log) Likelihood Sometimes?](https://stats.stackexchange.com/questions/141087/i-am-wondering-why-we-use-negative-log-likelihood-sometimes)  
This question puzzled me when I read page 96 of PRML. The original book uses likelihood, but in the errata, the author changed likelihood to negative likelihood to statisfy to condition of Robbins and Monro(1951). 

## [Completing The Square](https://learnbayes.org/index.php?option=com_content&view=article&id=77:completesquare&catid=83&Itemid=479&showall=1&limitstart=)  
In this reference tutorial, we will demonstrate how to complete the square in both univariate (scalar) and multivariate (matrix) contexts. You can also find relevant discussion in section 2.3.1 of PRML.

## [Normal distance from the origin to the dession surface](https://math.stackexchange.com/questions/1029153/deriving-the-normal-distance-from-the-origin-to-the-decision-surface)  

Derivation of PRML eq(4.5).  























